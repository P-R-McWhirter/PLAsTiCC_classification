{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import lightgbm as lgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.enable()\n",
    "\n",
    "train = pd.read_csv('training_set.csv')\n",
    "train['flux_ratio_sq'] = np.power(train['flux'] / train['flux_err'], 2.0)\n",
    "train['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n",
    "train_det = train.where(train['detected'] == 1)\n",
    "\n",
    "aggs = {\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum','skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}\n",
    "\n",
    "aggs_det = {\n",
    "    'mjd': ['min', 'max', 'size'],\n",
    "    'passband': ['mean', 'std', 'var'],\n",
    "}\n",
    "\n",
    "agg_train = train.groupby('object_id').agg(aggs)\n",
    "agg_train_det = train_det.groupby('object_id').agg(aggs_det)\n",
    "\n",
    "new_columns = [\n",
    "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "]\n",
    "new_columns_det = [\n",
    "    k + '_' + agg for k in aggs_det.keys() for agg in aggs_det[k]\n",
    "]\n",
    "\n",
    "agg_train.columns = new_columns\n",
    "agg_train['flux_diff'] = agg_train['flux_max'] - agg_train['flux_min']\n",
    "agg_train['flux_dif2'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_mean']\n",
    "agg_train['flux_w_mean'] = agg_train['flux_by_flux_ratio_sq_sum'] / agg_train['flux_ratio_sq_sum']\n",
    "agg_train['flux_dif3'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_w_mean']\n",
    "\n",
    "agg_train_det.columns = new_columns_det\n",
    "agg_train_det['mjd_diff'] = agg_train_det['mjd_max'] - agg_train_det['mjd_min']\n",
    "\n",
    "agg_train_det.columns = agg_train_det.columns + \"_det\"\n",
    "\n",
    "del agg_train_det['mjd_max_det'], agg_train_det['mjd_min_det']\n",
    "\n",
    "agg_train = pd.concat([agg_train, agg_train_det], axis=1, join='inner')\n",
    "\n",
    "del train, train_det, agg_train_det\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  object_id     flux_min    flux_max   flux_mean  flux_median  \\\n",
      "0      0        615 -1100.440063  660.626343 -123.096998   -89.477524   \n",
      "1      1        713   -14.735178   14.770886   -1.423351    -0.873033   \n",
      "2      2        730   -19.159811   47.310059    2.267434     0.409172   \n",
      "3      3        745   -15.494463  220.795212    8.909206     1.035895   \n",
      "4      4       1124   -16.543753  143.600189    7.145702     1.141288   \n",
      "\n",
      "     flux_std  flux_skew  flux_err_min  flux_err_max    ...             A_i  \\\n",
      "0  394.109851  -0.349540      2.130510     12.845472    ...     1286.644797   \n",
      "1    6.471144   0.014989      0.639458      9.115748    ...       10.320848   \n",
      "2    8.022239   3.177854      0.695106     11.281384    ...       39.820096   \n",
      "3   27.558208   4.979826      0.567170     55.892746    ...      214.925130   \n",
      "4   20.051722   4.406298      0.695277     11.383690    ...      146.543488   \n",
      "\n",
      "          B_i       t_0_i   t_fall_i   t_rise_i         A_z         B_z  \\\n",
      "0 -151.376593  489.852676  16.775578  -0.167843  284.903828 -164.201408   \n",
      "1   -3.896904  110.460130   5.138921  -5.090155   11.584726   -4.294105   \n",
      "2   -0.131664  751.641548  27.927757 -17.968431   61.102289    0.008791   \n",
      "3    0.311987  355.043174  27.371796  -0.000001  273.614693    0.594329   \n",
      "4    0.596142  721.834279  46.838737  -0.035366  152.794461    0.762271   \n",
      "\n",
      "        t_0_z    t_fall_z  t_rise_z  \n",
      "0  822.507955  308.016602 -2.058962  \n",
      "1  114.503105    4.786573 -4.704755  \n",
      "2  733.500001   29.987512 -9.716955  \n",
      "3  353.165896   35.005530 -1.473222  \n",
      "4  733.929069   37.200062 -0.036479  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "Unique classes :  [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n"
     ]
    }
   ],
   "source": [
    "meta_train = pd.read_csv('training_set_metadata_head_oldlab.csv')\n",
    "meta_train.head()\n",
    "\n",
    "sn_train = pd.read_csv('supernova_train.csv')\n",
    "sn_train.head()\n",
    "\n",
    "full_train = agg_train.reset_index().merge(\n",
    "    right=meta_train,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "full_train = full_train.reset_index().merge(\n",
    "    right=sn_train,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weight = {\n",
    "    c: 1 for c in classes\n",
    "}\n",
    "for c in [64, 15]:\n",
    "    class_weight[c] = 2\n",
    "\n",
    "print(full_train.head())\n",
    "print('Unique classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id'], full_train['distmod'], full_train['hostgal_specz'], full_train['mwebv']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'],full_train['gal_b'],full_train['ddf']\n",
    "    \n",
    "    \n",
    "train_mean = full_train.mean(axis=0)\n",
    "full_train.fillna(train_mean, inplace=True)\n",
    "print(len(train_mean))\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "full_train_new = full_train.copy()\n",
    "ss = StandardScaler()\n",
    "full_train_ss = ss.fit_transform(full_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def build_model(dropout_rate=0.25,activation='relu'):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//2,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//4,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//8,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_y = np.unique(y)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y])\n",
    "y_categorical = to_categorical(y_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_count = Counter(y_map)\n",
    "wtable = np.zeros((len(unique_y),))\n",
    "for i in range(len(unique_y)):\n",
    "    wtable[i] = y_count[i]/y_map.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(100, 49), b.shape=(49, 256), m=100, n=256, k=49\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-a6d4fd337d6a>\", line 10, in <module>\n    model = build_model(dropout_rate=0.5,activation='tanh')\n  File \"<ipython-input-9-8c5747ab576f>\", line 6, in build_model\n    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1072, in dot\n    out = tf.matmul(x, y)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(100, 49), b.shape=(49, 256), m=100, n=256, k=49\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(100, 49), b.shape=(49, 256), m=100, n=256, k=49\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a6d4fd337d6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mplot_loss_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(100, 49), b.shape=(49, 256), m=100, n=256, k=49\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-a6d4fd337d6a>\", line 10, in <module>\n    model = build_model(dropout_rate=0.5,activation='tanh')\n  File \"<ipython-input-9-8c5747ab576f>\", line 6, in build_model\n    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1072, in dot\n    out = tf.matmul(x, y)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(100, 49), b.shape=(49, 256), m=100, n=256, k=49\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "oof_preds = np.zeros((len(full_train_ss), len(classes)))\n",
    "epochs = 600\n",
    "batch_size = 100\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "    \n",
    "    model = build_model(dropout_rate=0.5,activation='tanh')\n",
    "    model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n",
    "    \n",
    "    plot_loss_acc(history)\n",
    "    \n",
    "    print('Loading Best Model')\n",
    "    model.load_weights('./keras.model')\n",
    "    # # Get predicted probabilities for each class\n",
    "    oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "    clfs.append(model)\n",
    "    \n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "class_names = list(sample_sub.columns[1:-1])\n",
    "del sample_sub;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_test = pd.read_csv('test_set_metadata.csv')\n",
    "\n",
    "import time\n",
    "\n",
    "headis = ['object_id','mjd','passband','flux','flux_err','detected']\n",
    "\n",
    "skips = [0, 9234918, 18406171, 27609073, 37041469, 46197790, 55770062, 64926393, 74082924, 83421236, 92557978, 102091641, 111228285,\n",
    "        120365006, 129501800, 137843882, 148172034, 157308879, 166445472, 175979221, 185116017, 194649736, 203786228, 212922916,\n",
    "        222059646, 231593331, 240730004, 249866875, 259003404, 268537163, 277673872, 286810516, 296344221, 305481051, 314617603,\n",
    "        324151351, 333288179, 342424765, 351958414, 361095248, 370231906, 379765602, 388902239, 398038933, 407175651, 416709263,\n",
    "        425846082, 434982658, 444119308, 453653104]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i_c in range(0, 49):\n",
    "    df = pd.read_csv('test_set.csv', names = headis, header=None, skiprows = skips[i_c]+1, nrows = (skips[i_c+1] - skips[i_c]), low_memory = False)\n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    df_det = df.where(df['detected'] == 1)\n",
    "    # Group by object id\n",
    "    agg_test = df.groupby('object_id').agg(aggs)\n",
    "    agg_test_det = df_det.groupby('object_id').agg(aggs_det)\n",
    "    agg_test.columns = new_columns\n",
    "    agg_test['flux_diff'] = agg_test['flux_max'] - agg_test['flux_min']\n",
    "    agg_test['flux_dif2'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_mean']\n",
    "    agg_test['flux_w_mean'] = agg_test['flux_by_flux_ratio_sq_sum'] / agg_test['flux_ratio_sq_sum']\n",
    "    agg_test['flux_dif3'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_w_mean']\n",
    "    agg_test_det.columns = new_columns_det\n",
    "    agg_test_det['mjd_diff'] = agg_test_det['mjd_max'] - agg_test_det['mjd_min']\n",
    "\n",
    "    agg_test_det.columns = agg_test_det.columns + \"_det\"\n",
    "    \n",
    "    del agg_test_det['mjd_max_det'], agg_test_det['mjd_min_det']\n",
    "    \n",
    "    agg_test = pd.concat([agg_test, agg_test_det], axis=1, join='inner')\n",
    "\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "    \n",
    "    # Merge with meta data\n",
    "    full_test = agg_test.reset_index().merge(\n",
    "        right=meta_test,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "    full_test[full_train.columns] = full_test[full_train.columns].fillna(train_mean)\n",
    "    full_test_ss = ss.transform(full_test[full_train.columns])\n",
    "    # Make predictions\n",
    "    preds = None\n",
    "    for clf in clfs:\n",
    "        if preds is None:\n",
    "            preds = clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "        else:\n",
    "            preds += clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "    \n",
    "   # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds.shape[0])\n",
    "    for i in range(preds.shape[1]):\n",
    "        preds_99 *= (1 - preds[:, i])\n",
    "    \n",
    "    # Store predictions\n",
    "    preds_df = pd.DataFrame(preds, columns=class_names)\n",
    "    preds_df['object_id'] = full_test['object_id']\n",
    "    preds_df['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    \n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('predictions_newnn.csv',  header=True, mode='a', index=False)\n",
    "    else: \n",
    "        preds_df.to_csv('predictions_newnn.csv',  header=False, mode='a', index=False)\n",
    "        \n",
    "    del agg_test, agg_test_det, full_test, preds_df, preds\n",
    "#     print('done')\n",
    "    print('%15d done in %5.1f' % ((i_c + 1), (time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = pd.read_csv('predictions_newnn.csv')\n",
    "\n",
    "print(z.groupby('object_id').size().max())\n",
    "print((z.groupby('object_id').size() > 1).sum())\n",
    "\n",
    "z = z.groupby('object_id').mean()\n",
    "\n",
    "def GenUnknown(data):\n",
    "    return ((((((data[\"mymedian\"]) + (((data[\"mymean\"]) / 2.0)))/2.0)) + (((((1.0) - (((data[\"mymax\"]) * (((data[\"mymax\"]) * (data[\"mymax\"]))))))) / 2.0)))/2.0)\n",
    "\n",
    "feats = ['class_6', 'class_15', 'class_16', 'class_42', 'class_52', 'class_53',\n",
    "         'class_62', 'class_64', 'class_65', 'class_67', 'class_88', 'class_90',\n",
    "         'class_92', 'class_95']\n",
    "\n",
    "y = z\n",
    "y['mymean'] = z[feats].mean(axis=1)\n",
    "y['mymedian'] = z[feats].median(axis=1)\n",
    "y['mymax'] = z[feats].max(axis=1)\n",
    "\n",
    "z['class_99'] = GenUnknown(y)\n",
    "\n",
    "z_new = z\n",
    "z_new[\"sum\"] = z_new.sum(axis=1)\n",
    "\n",
    "z_new = z_new.loc[:,\"class_6\":\"class_99\"].div(z_new[\"sum\"], axis=0)\n",
    "z_new.index = z_new.index.map(int)\n",
    "\n",
    "z_new.to_csv('single_predictions_newnn.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
