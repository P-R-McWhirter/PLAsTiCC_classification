{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Feature Extraction: 100%|████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.59it/s]\n",
      "Feature Extraction: 100%|████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.09it/s]\n",
      "Feature Extraction: 100%|████████████████████████████████████████████████| 20/20 [00:02<00:00,  9.84it/s]\n",
      "Feature Extraction: 100%|████████████████████████████████████████████████| 20/20 [00:01<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes : 14, [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "{6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
      "                                                   count          mean  \\\n",
      "flux_min                                          7848.0 -5.817377e+02   \n",
      "flux_max                                          7848.0  1.224459e+03   \n",
      "flux_mean                                         7848.0  3.351244e+01   \n",
      "flux_median                                       7848.0 -1.317095e+01   \n",
      "flux_std                                          7848.0  2.775506e+02   \n",
      "flux_skew                                         7848.0  2.158618e+00   \n",
      "flux_err_min                                      7848.0  2.936255e+00   \n",
      "flux_err_max                                      7848.0  4.104654e+02   \n",
      "flux_err_mean                                     7848.0  3.303208e+01   \n",
      "flux_err_median                                   7848.0  1.248738e+01   \n",
      "flux_err_std                                      7848.0  6.978792e+01   \n",
      "flux_err_skew                                     7848.0  2.483038e+00   \n",
      "detected_mean                                     7848.0  1.573900e-01   \n",
      "flux_ratio_sq_sum                                 7848.0  1.100639e+05   \n",
      "flux_ratio_sq_skew                                7848.0  6.669395e+00   \n",
      "flux_by_flux_ratio_sq_sum                         7848.0  4.718638e+07   \n",
      "flux_by_flux_ratio_sq_skew                        7848.0  5.916174e+00   \n",
      "flux_w_mean                                       7848.0  1.999271e+02   \n",
      "flux_diff1                                        7848.0  1.806196e+03   \n",
      "flux_diff2                                        7848.0  2.389844e+01   \n",
      "flux_diff3                                        7848.0  2.114325e+00   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  8.686732e+02   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.083683e+03   \n",
      "0__kurtosis                                       7841.0  2.931102e+00   \n",
      "0__skewness                                       7847.0  6.589448e-01   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  1.534345e+03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  9.258924e+02   \n",
      "1__kurtosis                                       7845.0  5.833965e+00   \n",
      "1__skewness                                       7848.0  1.343296e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.706631e+03   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.587983e+03   \n",
      "2__kurtosis                                       7848.0  7.780067e+00   \n",
      "2__skewness                                       7848.0  1.764522e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.855174e+03   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.597846e+03   \n",
      "3__kurtosis                                       7848.0  6.708265e+00   \n",
      "3__skewness                                       7848.0  1.611161e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  4.109145e+03   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.023273e+03   \n",
      "4__kurtosis                                       7848.0  6.259162e+00   \n",
      "4__skewness                                       7848.0  1.477267e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  5.195574e+03   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.436035e+03   \n",
      "5__kurtosis                                       7848.0  4.116877e+00   \n",
      "5__skewness                                       7848.0  9.237385e-01   \n",
      "flux__length                                      7848.0  1.811551e+02   \n",
      "flux__longest_strike_above_mean                   7848.0  1.418629e+01   \n",
      "flux__longest_strike_below_mean                   7848.0  3.506651e+01   \n",
      "flux__mean_abs_change                             7848.0  1.588398e+02   \n",
      "flux__mean_change                                 7848.0  3.320639e-01   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  7848.0  1.057250e+01   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7848.0  9.090813e+01   \n",
      "mjd__mean_abs_change                              7848.0  4.411311e+01   \n",
      "mjd__mean_change                                  7848.0  4.411311e+01   \n",
      "mjd_diff_det                                      7848.0  2.838325e+02   \n",
      "hostgal_photoz                                    7848.0  3.578845e-01   \n",
      "hostgal_photoz_err                                7848.0  1.556947e-01   \n",
      "distmod                                           5523.0  4.126396e+01   \n",
      "mwebv                                             7848.0  8.195260e-02   \n",
      "haversine                                         7848.0  1.493521e+00   \n",
      "latlon1                                           7848.0 -3.876255e-01   \n",
      "hostgal_photoz_certain                            7848.0  5.785227e-01   \n",
      "\n",
      "                                                           std           min  \\\n",
      "flux_min                                          1.342036e+04 -1.149388e+06   \n",
      "flux_max                                          2.867330e+04 -7.517203e+02   \n",
      "flux_mean                                         1.827033e+03 -4.783272e+04   \n",
      "flux_median                                       8.179198e+02 -4.336586e+04   \n",
      "flux_std                                          3.890649e+03  2.687631e+00   \n",
      "flux_skew                                         3.028822e+00 -1.281353e+01   \n",
      "flux_err_min                                      7.696597e+00  4.637530e-01   \n",
      "flux_err_max                                      2.538694e+04  9.063369e+00   \n",
      "flux_err_mean                                     1.199017e+03  2.198749e+00   \n",
      "flux_err_median                                   4.874503e+01  1.754588e+00   \n",
      "flux_err_std                                      4.112916e+03  9.761724e-01   \n",
      "flux_err_skew                                     2.966362e+00 -2.042322e+00   \n",
      "detected_mean                                     2.233782e-01  5.681818e-03   \n",
      "flux_ratio_sq_sum                                 5.943836e+05  1.269791e+02   \n",
      "flux_ratio_sq_skew                                3.110736e+00 -1.239108e+00   \n",
      "flux_by_flux_ratio_sq_sum                         3.074160e+09 -1.153257e+11   \n",
      "flux_by_flux_ratio_sq_skew                        5.626113e+00 -1.840455e+01   \n",
      "flux_w_mean                                       2.658033e+03 -6.990388e+04   \n",
      "flux_diff1                                        4.160635e+04  2.126078e+01   \n",
      "flux_diff2                                        1.436849e+03 -6.647960e+04   \n",
      "flux_diff3                                        6.559672e+01 -3.335919e+03   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           2.558435e+04  6.706000e-03   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           5.287930e+04  1.739167e-01   \n",
      "0__kurtosis                                       7.578568e+00 -3.203161e+00   \n",
      "0__skewness                                       1.560550e+00 -7.756287e+00   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           1.159396e+04  1.081000e-03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7.143211e+03  1.861472e-01   \n",
      "1__kurtosis                                       8.837887e+00 -4.291602e+00   \n",
      "1__skewness                                       1.977723e+00 -7.384496e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           4.382792e+04  1.954300e-02   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           2.822033e+04  2.367083e-01   \n",
      "2__kurtosis                                       9.619066e+00 -2.425456e+00   \n",
      "2__skewness                                       2.131314e+00 -7.614107e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           4.222836e+04  1.745100e-02   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           2.554205e+04  1.149140e+00   \n",
      "3__kurtosis                                       8.775497e+00 -2.145722e+00   \n",
      "3__skewness                                       1.986927e+00 -7.445896e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           5.735607e+04  4.180100e-02   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           3.189314e+04  2.536212e+00   \n",
      "4__kurtosis                                       8.552450e+00 -1.945264e+00   \n",
      "4__skewness                                       1.905686e+00 -7.339672e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           8.173287e+04  8.507800e-02   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           3.686801e+04  4.187342e+00   \n",
      "5__kurtosis                                       7.396436e+00 -2.206612e+00   \n",
      "5__skewness                                       1.630383e+00 -7.184298e+00   \n",
      "flux__length                                      9.175221e+01  4.700000e+01   \n",
      "flux__longest_strike_above_mean                   1.482250e+01  1.000000e+00   \n",
      "flux__longest_strike_below_mean                   4.844920e+01  1.000000e+00   \n",
      "flux__mean_abs_change                             1.700030e+03  2.381600e+00   \n",
      "flux__mean_change                                 2.748721e+01 -4.387450e+02   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  1.941361e+01  1.000000e+00   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7.223568e+01  1.000000e+00   \n",
      "mjd__mean_abs_change                              1.081050e+02  8.700000e-03   \n",
      "mjd__mean_change                                  1.081050e+02  8.700000e-03   \n",
      "mjd_diff_det                                      3.480104e+02  1.090000e-02   \n",
      "hostgal_photoz                                    5.455516e-01  0.000000e+00   \n",
      "hostgal_photoz_err                                3.003674e-01  0.000000e+00   \n",
      "distmod                                           2.262711e+00  3.199610e+01   \n",
      "mwebv                                             1.505977e-01  3.000000e-03   \n",
      "haversine                                         5.827418e-01  4.282284e-02   \n",
      "latlon1                                           3.157551e+00 -6.121150e+00   \n",
      "hostgal_photoz_certain                            1.329522e+00  0.000000e+00   \n",
      "\n",
      "                                                           25%            50%  \\\n",
      "flux_min                                            -97.748140     -63.014893   \n",
      "flux_max                                             92.712827     166.902748   \n",
      "flux_mean                                             1.952291       7.054367   \n",
      "flux_median                                           0.233296       0.993680   \n",
      "flux_std                                             22.534987      35.915890   \n",
      "flux_skew                                             0.656051       2.149175   \n",
      "flux_err_min                                          1.046307       1.516784   \n",
      "flux_err_max                                         45.324589      54.928312   \n",
      "flux_err_mean                                         4.421238      12.828899   \n",
      "flux_err_median                                       3.257738       9.393351   \n",
      "flux_err_std                                          4.231706      10.814982   \n",
      "flux_err_skew                                         1.213292       1.499814   \n",
      "detected_mean                                         0.029814       0.071429   \n",
      "flux_ratio_sq_sum                                  1163.388509    4659.903047   \n",
      "flux_ratio_sq_skew                                    4.600050       6.557691   \n",
      "flux_by_flux_ratio_sq_sum                         18387.658261  245633.617776   \n",
      "flux_by_flux_ratio_sq_skew                            4.371290       7.062250   \n",
      "flux_w_mean                                          25.136668      83.231919   \n",
      "flux_diff1                                          160.170960     255.436458   \n",
      "flux_diff2                                           12.395337      22.569265   \n",
      "flux_diff3                                            1.494130       1.986227   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"              18.305239      48.102023   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"              23.458444      46.555761   \n",
      "0__kurtosis                                          -0.376250       0.634380   \n",
      "0__skewness                                          -0.246774       0.316416   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"              17.174671      79.951323   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"              15.636640      64.729119   \n",
      "1__kurtosis                                           0.105205       2.999472   \n",
      "1__skewness                                           0.038536       1.276447   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"              76.046479     246.346179   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"              63.231898     198.328051   \n",
      "2__kurtosis                                           0.802989       4.821736   \n",
      "2__skewness                                           0.427805       1.947666   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"              93.025052     275.749970   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"              79.165062     222.339731   \n",
      "3__kurtosis                                           0.645124       3.949253   \n",
      "3__skewness                                           0.357315       1.817056   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"             144.869882     359.393182   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"             129.255762     301.527388   \n",
      "4__kurtosis                                           0.641737       3.415452   \n",
      "4__skewness                                           0.297040       1.609730   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"             149.471881     367.008601   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"             166.007396     330.686203   \n",
      "5__kurtosis                                           0.163575       1.504293   \n",
      "5__skewness                                           0.013215       0.718874   \n",
      "flux__length                                        122.000000     136.000000   \n",
      "flux__longest_strike_above_mean                       7.000000      10.000000   \n",
      "flux__longest_strike_below_mean                       9.000000      16.000000   \n",
      "flux__mean_abs_change                                15.011102      20.554584   \n",
      "flux__mean_change                                    -0.101453       0.001611   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean      3.000000       5.000000   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean     44.000000      79.500000   \n",
      "mjd__mean_abs_change                                  4.218475       7.842496   \n",
      "mjd__mean_change                                      4.218475       7.842496   \n",
      "mjd_diff_det                                         40.882500      95.774050   \n",
      "hostgal_photoz                                        0.000000       0.210300   \n",
      "hostgal_photoz_err                                    0.000000       0.018000   \n",
      "distmod                                              39.845250      41.167900   \n",
      "mwebv                                                 0.018000       0.032000   \n",
      "haversine                                             1.164021       1.690880   \n",
      "latlon1                                              -2.941054      -0.987176   \n",
      "hostgal_photoz_certain                                0.000000       0.236039   \n",
      "\n",
      "                                                           75%           max  \n",
      "flux_min                                         -3.720073e+01  5.109941e+02  \n",
      "flux_max                                          3.650707e+02  2.432809e+06  \n",
      "flux_mean                                         1.746234e+01  1.403881e+05  \n",
      "flux_median                                       2.535580e+00  3.087974e+04  \n",
      "flux_std                                          7.584766e+01  2.796894e+05  \n",
      "flux_skew                                         3.441721e+00  1.849729e+01  \n",
      "flux_err_min                                      2.166468e+00  3.276297e+02  \n",
      "flux_err_max                                      6.349475e+01  2.234069e+06  \n",
      "flux_err_mean                                     1.551099e+01  1.047130e+05  \n",
      "flux_err_median                                   1.195244e+01  3.853605e+03  \n",
      "flux_err_std                                      1.239876e+01  3.605993e+05  \n",
      "flux_err_skew                                     1.803128e+00  1.378938e+01  \n",
      "detected_mean                                     1.666667e-01  1.000000e+00  \n",
      "flux_ratio_sq_sum                                 2.592588e+04  1.086076e+07  \n",
      "flux_ratio_sq_skew                                8.634171e+00  1.873359e+01  \n",
      "flux_by_flux_ratio_sq_sum                         2.979438e+06  1.570616e+11  \n",
      "flux_by_flux_ratio_sq_skew                        9.486608e+00  1.876046e+01  \n",
      "flux_w_mean                                       2.013387e+02  1.793980e+05  \n",
      "flux_diff1                                        5.078297e+02  3.582197e+06  \n",
      "flux_diff2                                        4.301897e+01  6.077639e+04  \n",
      "flux_diff3                                        3.085920e+00  2.836464e+03  \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           1.675728e+02  2.230438e+06  \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           1.393153e+02  4.631342e+06  \n",
      "0__kurtosis                                       2.905992e+00  7.155293e+01  \n",
      "0__skewness                                       1.276045e+00  8.446201e+00  \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           3.871591e+02  6.352250e+05  \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           2.915286e+02  4.443702e+05  \n",
      "1__kurtosis                                       8.990190e+00  5.777428e+01  \n",
      "1__skewness                                       2.842803e+00  7.593994e+00  \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           8.201768e+02  3.741323e+06  \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           6.110753e+02  2.428124e+06  \n",
      "2__kurtosis                                       1.183584e+01  5.798279e+01  \n",
      "2__skewness                                       3.143396e+00  7.609116e+00  \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           8.380184e+02  3.265140e+06  \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           6.382118e+02  2.145546e+06  \n",
      "3__kurtosis                                       9.605804e+00  5.771809e+01  \n",
      "3__skewness                                       2.848140e+00  7.588468e+00  \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           9.435420e+02  3.667709e+06  \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7.284919e+02  2.625974e+06  \n",
      "4__kurtosis                                       8.633458e+00  5.792441e+01  \n",
      "4__skewness                                       2.623668e+00  7.608456e+00  \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           9.050253e+02  4.046475e+06  \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7.192184e+02  2.876665e+06  \n",
      "5__kurtosis                                       4.586256e+00  5.610575e+01  \n",
      "5__skewness                                       1.783578e+00  7.466092e+00  \n",
      "flux__length                                      2.550000e+02  3.520000e+02  \n",
      "flux__longest_strike_above_mean                   1.600000e+01  2.310000e+02  \n",
      "flux__longest_strike_below_mean                   3.525000e+01  3.310000e+02  \n",
      "flux__mean_abs_change                             3.197212e+01  1.172606e+05  \n",
      "flux__mean_change                                 1.184764e-01  2.246845e+03  \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  8.000000e+00  2.830000e+02  \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  1.140000e+02  3.460000e+02  \n",
      "mjd__mean_abs_change                              1.900680e+01  1.071030e+03  \n",
      "mjd__mean_change                                  1.900680e+01  1.071030e+03  \n",
      "mjd_diff_det                                      4.740591e+02  1.092845e+03  \n",
      "hostgal_photoz                                    4.312000e-01  2.999400e+00  \n",
      "hostgal_photoz_err                                1.223750e-01  1.734800e+00  \n",
      "distmod                                           4.239855e+01  4.702560e+01  \n",
      "mwebv                                             7.600000e-02  2.747000e+00  \n",
      "haversine                                         1.978942e+00  2.119827e+00  \n",
      "latlon1                                           2.311904e+00  8.805560e+00  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hostgal_photoz_certain                            5.619134e-01  1.192976e+01  \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(100, 62), b.shape=(62, 512), m=100, n=512, k=62\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-067b4447bfc4>\", line 747, in <module>\n    main(len(sys.argv), sys.argv)\n  File \"<ipython-input-1-067b4447bfc4>\", line 698, in main\n    model = build_model(dropout_rate=0.5,activation='tanh')\n  File \"<ipython-input-1-067b4447bfc4>\", line 462, in build_model\n    model.add(Dense(start_neurons, input_dim=62, activation=activation))\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1072, in dot\n    out = tf.matmul(x, y)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(100, 62), b.shape=(62, 512), m=100, n=512, k=62\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(100, 62), b.shape=(62, 512), m=100, n=512, k=62\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-067b4447bfc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-067b4447bfc4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(argc, argv)\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m                     batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mplot_loss_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(100, 62), b.shape=(62, 512), m=100, n=512, k=62\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-067b4447bfc4>\", line 747, in <module>\n    main(len(sys.argv), sys.argv)\n  File \"<ipython-input-1-067b4447bfc4>\", line 698, in main\n    model = build_model(dropout_rate=0.5,activation='tanh')\n  File \"<ipython-input-1-067b4447bfc4>\", line 462, in build_model\n    model.add(Dense(start_neurons, input_dim=62, activation=activation))\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 467, in add\n    layer(x)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1072, in dot\n    out = tf.matmul(x, y)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ross\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(100, 62), b.shape=(62, 512), m=100, n=512, k=62\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_dense_1_input_0_0/_217, dense_1/kernel/read)]]\n\t [[Node: training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape/_307 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1115_training/Adam/gradients/loss/dense_5_loss/clip_by_value/Minimum_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is forked from iprapas's notebook \n",
    "https://www.kaggle.com/iprapas/ideas-from-kernels-and-discussion-lb-1-135\n",
    "\n",
    "#    https://www.kaggle.com/ogrellier/plasticc-in-a-kernel-meta-and-data\n",
    "#    https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70908\n",
    "#    https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification\n",
    "#\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import gc; gc.enable()\n",
    "from functools import partial, wraps\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "@jit\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq, \n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n",
    "        index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux], axis=1)\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "    \n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,       \n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux_agg], axis=1)\n",
    "    \n",
    "\n",
    "def featurize(df, df_meta, aggs, fcp, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "    \n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df = process_flux_agg(agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(df, \n",
    "                                               column_id='object_id', \n",
    "                                               column_sort='mjd', \n",
    "                                               column_kind='passband', \n",
    "                                               column_value='flux', \n",
    "                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux', \n",
    "                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux_by_flux_ratio_sq', \n",
    "                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "    agg_df_mjd = extract_features(df_det, \n",
    "                                  column_id='object_id', \n",
    "                                  column_value='mjd', \n",
    "                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    \n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True) \n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)      \n",
    "    agg_df_ts = pd.concat([agg_df, \n",
    "                           agg_df_ts_flux_passband, \n",
    "                           agg_df_ts_flux, \n",
    "                           agg_df_ts_flux_by_flux_ratio_sq, \n",
    "                           agg_df_mjd], axis=1).reset_index()\n",
    "    \n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_meta(filename):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "    \n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df\n",
    "\n",
    "\n",
    "def multi_weighted_logloss_old(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}    \n",
    "    \n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgbm_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"  \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def xgb_multi_weighted_logloss(y_predicted, y_true, classes, class_weights):\n",
    "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted, \n",
    "                                  classes, class_weights)\n",
    "    return 'wloss', loss\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    return importances_\n",
    "\n",
    "\n",
    "def xgb_modeling_cross_validation(params,\n",
    "                                  full_train, \n",
    "                                  y, \n",
    "                                  classes, \n",
    "                                  class_weights, \n",
    "                                  nr_fold=5, \n",
    "                                  random_state=1):\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    # loss function\n",
    "    func_loss = partial(xgb_multi_weighted_logloss, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights)\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=func_loss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('xgb_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train, \n",
    "                                   y, \n",
    "                                   classes, \n",
    "                                   class_weights, \n",
    "                                   nr_fold=5, \n",
    "                                   random_state=1):\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgbm_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('lgbm_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean):\n",
    "    \n",
    "    # process all features\n",
    "    full_test = featurize(df_, meta_, \n",
    "                          featurize_configs['aggs'], \n",
    "                          featurize_configs['fcp'])\n",
    "    full_test.fillna(0, inplace=True)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features])\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features])\n",
    "            \n",
    "    preds_ = preds_ / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, \n",
    "                             columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def process_test(clfs, \n",
    "                 features, \n",
    "                 featurize_configs, \n",
    "                 train_mean,\n",
    "                 filename='predictions_xgb.csv',\n",
    "                 chunks=5000000):\n",
    "    start = time.time()\n",
    "\n",
    "    meta_test = process_meta('test_set_metadata.csv')\n",
    "    # meta_test.set_index('object_id',inplace=True)\n",
    "\n",
    "    remain_df = None\n",
    "    for i_c, df in enumerate(pd.read_csv('test_set.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "        \n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "        if remain_df is None:\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "        \n",
    "        preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clfs,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=features,\n",
    "                                 featurize_configs=featurize_configs,\n",
    "                                 train_mean=train_mean)\n",
    "    \n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv(filename, header=True, mode='a', index=False)\n",
    "        else:\n",
    "            preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    \n",
    "        del preds_df\n",
    "        gc.collect()\n",
    "        print('{:15d} done in {:5.1f} minutes' .format(\n",
    "                chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "        \n",
    "    # Compute last object in remain_df\n",
    "    preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=features,\n",
    "                             featurize_configs=featurize_configs,\n",
    "                             train_mean=train_mean)\n",
    "        \n",
    "    preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(dropout_rate=0.25,activation='relu'):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=62, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//2,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//4,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//8,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "def main(argc, argv):\n",
    "    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity    \n",
    "    \n",
    "    # agg features\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "    \n",
    "    # tsfresh features\n",
    "    fcp = {\n",
    "        'flux': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "            'length': None,\n",
    "        },\n",
    "                \n",
    "        'flux_by_flux_ratio_sq': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,       \n",
    "        },\n",
    "                \n",
    "        'flux_passband': {\n",
    "            'fft_coefficient': [\n",
    "                    {'coeff': 0, 'attr': 'abs'}, \n",
    "                    {'coeff': 1, 'attr': 'abs'}\n",
    "                ],\n",
    "            'kurtosis' : None, \n",
    "            'skewness' : None,\n",
    "        },\n",
    "                \n",
    "        'mjd': {\n",
    "            'maximum': None, \n",
    "            'minimum': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    best_params = {\n",
    "            'device': 'cpu', \n",
    "            'objective': 'multiclass', \n",
    "            'num_class': 14, \n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': -1, \n",
    "            'max_depth': 7, \n",
    "            'n_estimators': 500, \n",
    "            'subsample_freq': 2, \n",
    "            'subsample_for_bin': 5000, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 1.0, \n",
    "            'cat_smooth': 59.5, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'metric': 'multi_logloss', \n",
    "            'xgboost_dart_mode': False, \n",
    "            'uniform_drop': False, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'drop_rate': 0.173, \n",
    "            'learning_rate': 0.0267, \n",
    "            'max_drop': 5, \n",
    "            'min_child_samples': 10, \n",
    "            'min_child_weight': 100.0, \n",
    "            'min_split_gain': 0.1, \n",
    "            'num_leaves': 7, \n",
    "            'reg_alpha': 0.1, \n",
    "            'reg_lambda': 0.00023, \n",
    "            'skip_drop': 0.44, \n",
    "            'subsample': 0.75}\n",
    "\n",
    "    meta_train = process_meta('training_set_metadata_head_oldlab.csv')\n",
    "    \n",
    "    train = pd.read_csv('training_set.csv')\n",
    "    full_train = featurize(train, meta_train, aggs, fcp)\n",
    "\n",
    "    if 'target' in full_train:\n",
    "        y = full_train['target']\n",
    "        del full_train['target']\n",
    "        \n",
    "    classes = sorted(y.unique())    \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    class_weights = {c: 1 for c in classes}\n",
    "    class_weights.update({c:2 for c in [64, 15]})\n",
    "    print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "    print(class_weights)\n",
    "    #sanity check: classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    #sanity check: class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    #if len(np.unique(y_true)) > 14:\n",
    "    #    classes.append(99)\n",
    "    #    class_weights[99] = 2\n",
    "    \n",
    "    if 'object_id' in full_train:\n",
    "        oof_df = full_train[['object_id']]\n",
    "        del full_train['object_id'] \n",
    "        #del full_train['distmod'] \n",
    "        del full_train['hostgal_specz']\n",
    "        del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "        del full_train['ddf']\n",
    "    \n",
    "    train_mean = full_train.mean(axis=0)\n",
    "    #train_mean.to_hdf('train_data.hdf5', 'data')\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    print(full_train.describe().T)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    full_train.fillna(0, inplace=True)\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    full_train_new = full_train.copy()\n",
    "    ss = StandardScaler()\n",
    "    full_train_ss = ss.fit_transform(full_train_new)\n",
    "    \n",
    "    #print(full_train_ss.shape[1])\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i,val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "        \n",
    "    y_map = np.zeros((y.shape[0],))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "    y_categorical = to_categorical(y_map)\n",
    "    \n",
    "    y_count = Counter(y_map)\n",
    "    wtable = np.zeros((len(unique_y),))\n",
    "    for i in range(len(unique_y)):\n",
    "        wtable[i] = y_count[i]/y_map.shape[0]\n",
    "        \n",
    "    def mywloss(y_true,y_pred):  \n",
    "        yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "        loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "        return loss\n",
    "\n",
    "    clfs = []\n",
    "    oof_preds = np.zeros((len(full_train_ss), 14))\n",
    "    epochs = 600\n",
    "    batch_size = 100\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "        checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "        x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "        x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "        y_train_flat = y_map[trn_]\n",
    "        y_valid_flat = y_map[val_]\n",
    "        \n",
    "        sm = SMOTE(k_neighbors=7, n_jobs=4, random_state=42)\n",
    "        x_train, y_train_flat = sm.fit_resample(x_train, y_train_flat)\n",
    "        x_train = pd.DataFrame(x_train, columns=full_train.columns)\n",
    "        y_train_flat = pd.Series(y_train_flat)\n",
    "        \n",
    "        y_train = to_categorical(y_train_flat)\n",
    "        y_valid = to_categorical(y_valid_flat)\n",
    "    \n",
    "        model = build_model(dropout_rate=0.5,activation='tanh')    \n",
    "        model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n",
    "    \n",
    "        plot_loss_acc(history)\n",
    "    \n",
    "        print('Loading Best Model')\n",
    "        model.load_weights('./keras.model')\n",
    "        # # Get predicted probabilities for each class\n",
    "        oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "        print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "        clfs.append(model)\n",
    "    \n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    sample_sub = pd.read_csv('sample_submission.csv')\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub;gc.collect()\n",
    "    \n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(12,12))\n",
    "    foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')\n",
    "        \n",
    "    filename = 'new_nn_method.csv'\n",
    "    print('save to {}'.format(filename))\n",
    "    # TEST\n",
    "    process_test(clfs, \n",
    "                 features=full_train.columns, \n",
    "                 featurize_configs={'aggs': aggs, 'fcp': fcp}, \n",
    "                 train_mean=train_mean, \n",
    "                 filename=filename,\n",
    "                 chunks=5000000)\n",
    "        \n",
    "    z = pd.read_csv(filename)\n",
    "    print(\"Shape BEFORE grouping: {}\".format(z.shape))\n",
    "    z = z.groupby('object_id').mean()\n",
    "    print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "    z.to_csv('single_{}'.format(filename), index=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(len(sys.argv), sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_csv('single_{}'.format(filename))\n",
    "\n",
    "print(z.groupby('object_id').size().max())\n",
    "print((z.groupby('object_id').size() > 1).sum())\n",
    "\n",
    "z = z.groupby('object_id').mean()\n",
    "\n",
    "z_new = z\n",
    "z_new[\"sum\"] = z_new.sum(axis=1)\n",
    "\n",
    "z_new = z_new.loc[:,\"class_6\":\"class_99\"].div(z_new[\"sum\"], axis=0)\n",
    "z_new.index = z_new.index.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = pd.read_csv('predictions.csv')\n",
    "\n",
    "print(z2.groupby('object_id').size().max())\n",
    "print((z2.groupby('object_id').size() > 1).sum())\n",
    "\n",
    "z2 = z2.groupby('object_id').mean()\n",
    "\n",
    "z2_new = z2\n",
    "z2_new[\"sum\"] = z2_new.sum(axis=1)\n",
    "\n",
    "z2_new = z2_new.loc[:,\"class_6\":\"class_99\"].div(z2_new[\"sum\"], axis=0)\n",
    "z2_new.index = z2_new.index.map(int)\n",
    "\n",
    "zf = (z_new + z2_new) / 2\n",
    "\n",
    "zf.to_csv('single_combined_predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
