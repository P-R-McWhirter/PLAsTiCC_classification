{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 15)\n",
      "(7848, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:07<00:00,  2.65it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:02<00:00,  8.21it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:01<00:00, 10.27it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:01<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes : 14, [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "{6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
      "     flux_min    flux_max  flux_mean  flux_median   flux_std  flux_skew  \\\n",
      "0 -581.737739  1224.45869  33.512438   -13.170947  277.55058   2.158618   \n",
      "\n",
      "   flux_err_min  flux_err_max  flux_err_mean  flux_err_median    ...     \\\n",
      "0      2.936255    410.465386       33.03208        12.487381    ...      \n",
      "\n",
      "            A_i         B_i       t_0_i      t_fall_i   t_rise_i  \\\n",
      "0  7.388705e+08 -722.430678  405.137333  1.308787e+07 -18.751744   \n",
      "\n",
      "            A_z          B_z       t_0_z      t_fall_z  t_rise_z  \n",
      "0  7.452673e+19 -1127.913651  449.104991  1.121607e+07 -29.10336  \n",
      "\n",
      "[1 rows x 83 columns]\n",
      "                                                   count          mean  \\\n",
      "flux_min                                          7848.0 -5.817377e+02   \n",
      "flux_max                                          7848.0  1.224459e+03   \n",
      "flux_mean                                         7848.0  3.351244e+01   \n",
      "flux_median                                       7848.0 -1.317095e+01   \n",
      "flux_std                                          7848.0  2.775506e+02   \n",
      "flux_skew                                         7848.0  2.158618e+00   \n",
      "flux_err_min                                      7848.0  2.936255e+00   \n",
      "flux_err_max                                      7848.0  4.104654e+02   \n",
      "flux_err_mean                                     7848.0  3.303208e+01   \n",
      "flux_err_median                                   7848.0  1.248738e+01   \n",
      "flux_err_std                                      7848.0  6.978792e+01   \n",
      "flux_err_skew                                     7848.0  2.483038e+00   \n",
      "detected_mean                                     7848.0  1.573900e-01   \n",
      "flux_ratio_sq_sum                                 7848.0  1.100639e+05   \n",
      "flux_ratio_sq_skew                                7848.0  6.669395e+00   \n",
      "flux_by_flux_ratio_sq_sum                         7848.0  4.718638e+07   \n",
      "flux_by_flux_ratio_sq_skew                        7848.0  5.916174e+00   \n",
      "flux_w_mean                                       7848.0  1.999271e+02   \n",
      "flux_diff1                                        7848.0  1.806196e+03   \n",
      "flux_diff2                                        7848.0  2.389844e+01   \n",
      "flux_diff3                                        7848.0  2.114325e+00   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  8.686732e+02   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.083683e+03   \n",
      "0__kurtosis                                       7841.0  2.931102e+00   \n",
      "0__skewness                                       7847.0  6.589448e-01   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  1.534345e+03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  9.258924e+02   \n",
      "1__kurtosis                                       7845.0  5.833965e+00   \n",
      "1__skewness                                       7848.0  1.343296e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.706631e+03   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.587983e+03   \n",
      "2__kurtosis                                       7848.0  7.780067e+00   \n",
      "2__skewness                                       7848.0  1.764522e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.855174e+03   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.597846e+03   \n",
      "3__kurtosis                                       7848.0  6.708265e+00   \n",
      "3__skewness                                       7848.0  1.611161e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  4.109145e+03   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.023273e+03   \n",
      "4__kurtosis                                       7848.0  6.259162e+00   \n",
      "4__skewness                                       7848.0  1.477267e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  5.195574e+03   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.436035e+03   \n",
      "5__kurtosis                                       7848.0  4.116877e+00   \n",
      "5__skewness                                       7848.0  9.237385e-01   \n",
      "flux__length                                      7848.0  1.811551e+02   \n",
      "flux__longest_strike_above_mean                   7848.0  1.418629e+01   \n",
      "flux__longest_strike_below_mean                   7848.0  3.506651e+01   \n",
      "flux__mean_abs_change                             7848.0  1.588398e+02   \n",
      "flux__mean_change                                 7848.0  3.320639e-01   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  7848.0  1.057250e+01   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7848.0  9.090813e+01   \n",
      "mjd__mean_abs_change                              7848.0  4.411311e+01   \n",
      "mjd__mean_change                                  7848.0  4.411311e+01   \n",
      "mjd_diff_det                                      7848.0  2.838325e+02   \n",
      "index                                             7848.0  3.923500e+03   \n",
      "hostgal_photoz                                    7848.0  3.578845e-01   \n",
      "hostgal_photoz_err                                7848.0  1.556947e-01   \n",
      "distmod                                           5523.0  4.126396e+01   \n",
      "mwebv                                             7848.0  8.195260e-02   \n",
      "haversine                                         7848.0  1.493521e+00   \n",
      "latlon1                                           7848.0 -3.876255e-01   \n",
      "hostgal_photoz_certain                            7848.0  5.785227e-01   \n",
      "A_g                                               7836.0  6.172404e+22   \n",
      "B_g                                               7836.0 -6.165571e+01   \n",
      "t_0_g                                             7836.0  4.753052e+02   \n",
      "t_fall_g                                          7836.0  1.228109e+06   \n",
      "t_rise_g                                          7836.0 -1.227240e+01   \n",
      "A_r                                               7838.0  1.175485e+22   \n",
      "B_r                                               7838.0 -1.584459e+03   \n",
      "t_0_r                                             7838.0  4.293598e+02   \n",
      "t_fall_r                                          7838.0  2.301616e+06   \n",
      "t_rise_r                                          7838.0 -2.492369e+01   \n",
      "A_i                                               7848.0  7.388705e+08   \n",
      "B_i                                               7848.0 -7.224307e+02   \n",
      "t_0_i                                             7848.0  4.051373e+02   \n",
      "t_fall_i                                          7848.0  1.308787e+07   \n",
      "t_rise_i                                          7848.0 -1.875174e+01   \n",
      "A_z                                               7848.0  7.452673e+19   \n",
      "B_z                                               7848.0 -1.127914e+03   \n",
      "t_0_z                                             7848.0  4.491050e+02   \n",
      "t_fall_z                                          7848.0  1.121607e+07   \n",
      "t_rise_z                                          7848.0 -2.910336e+01   \n",
      "\n",
      "                                                           std           min  \\\n",
      "flux_min                                          1.342036e+04 -1.149388e+06   \n",
      "flux_max                                          2.867330e+04 -7.517203e+02   \n",
      "flux_mean                                         1.827033e+03 -4.783272e+04   \n",
      "flux_median                                       8.179198e+02 -4.336586e+04   \n",
      "flux_std                                          3.890649e+03  2.687631e+00   \n",
      "flux_skew                                         3.028822e+00 -1.281353e+01   \n",
      "flux_err_min                                      7.696597e+00  4.637530e-01   \n",
      "flux_err_max                                      2.538694e+04  9.063369e+00   \n",
      "flux_err_mean                                     1.199017e+03  2.198749e+00   \n",
      "flux_err_median                                   4.874503e+01  1.754588e+00   \n",
      "flux_err_std                                      4.112916e+03  9.761724e-01   \n",
      "flux_err_skew                                     2.966362e+00 -2.042322e+00   \n",
      "detected_mean                                     2.233782e-01  5.681818e-03   \n",
      "flux_ratio_sq_sum                                 5.943836e+05  1.269791e+02   \n",
      "flux_ratio_sq_skew                                3.110736e+00 -1.239108e+00   \n",
      "flux_by_flux_ratio_sq_sum                         3.074160e+09 -1.153257e+11   \n",
      "flux_by_flux_ratio_sq_skew                        5.626113e+00 -1.840455e+01   \n",
      "flux_w_mean                                       2.658033e+03 -6.990388e+04   \n",
      "flux_diff1                                        4.160635e+04  2.126078e+01   \n",
      "flux_diff2                                        1.436849e+03 -6.647960e+04   \n",
      "flux_diff3                                        6.559672e+01 -3.335919e+03   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           2.558435e+04  6.706000e-03   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           5.287930e+04  1.739167e-01   \n",
      "0__kurtosis                                       7.578568e+00 -3.203161e+00   \n",
      "0__skewness                                       1.560550e+00 -7.756287e+00   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           1.159396e+04  1.081000e-03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7.143211e+03  1.861472e-01   \n",
      "1__kurtosis                                       8.837887e+00 -4.291602e+00   \n",
      "1__skewness                                       1.977723e+00 -7.384496e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           4.382792e+04  1.954300e-02   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           2.822033e+04  2.367083e-01   \n",
      "2__kurtosis                                       9.619066e+00 -2.425456e+00   \n",
      "2__skewness                                       2.131314e+00 -7.614107e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           4.222836e+04  1.745100e-02   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           2.554205e+04  1.149140e+00   \n",
      "3__kurtosis                                       8.775497e+00 -2.145722e+00   \n",
      "3__skewness                                       1.986927e+00 -7.445896e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           5.735607e+04  4.180100e-02   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           3.189314e+04  2.536212e+00   \n",
      "4__kurtosis                                       8.552450e+00 -1.945264e+00   \n",
      "4__skewness                                       1.905686e+00 -7.339672e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           8.173287e+04  8.507800e-02   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           3.686801e+04  4.187342e+00   \n",
      "5__kurtosis                                       7.396436e+00 -2.206612e+00   \n",
      "5__skewness                                       1.630383e+00 -7.184298e+00   \n",
      "flux__length                                      9.175221e+01  4.700000e+01   \n",
      "flux__longest_strike_above_mean                   1.482250e+01  1.000000e+00   \n",
      "flux__longest_strike_below_mean                   4.844920e+01  1.000000e+00   \n",
      "flux__mean_abs_change                             1.700030e+03  2.381600e+00   \n",
      "flux__mean_change                                 2.748721e+01 -4.387450e+02   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  1.941361e+01  1.000000e+00   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7.223568e+01  1.000000e+00   \n",
      "mjd__mean_abs_change                              1.081050e+02  8.700000e-03   \n",
      "mjd__mean_change                                  1.081050e+02  8.700000e-03   \n",
      "mjd_diff_det                                      3.480104e+02  1.090000e-02   \n",
      "index                                             2.265667e+03  0.000000e+00   \n",
      "hostgal_photoz                                    5.455516e-01  0.000000e+00   \n",
      "hostgal_photoz_err                                3.003674e-01  0.000000e+00   \n",
      "distmod                                           2.262711e+00  3.199610e+01   \n",
      "mwebv                                             1.505977e-01  3.000000e-03   \n",
      "haversine                                         5.827418e-01  4.282284e-02   \n",
      "latlon1                                           3.157551e+00 -6.121150e+00   \n",
      "hostgal_photoz_certain                            1.329522e+00  0.000000e+00   \n",
      "A_g                                               5.463872e+24 -1.902536e+07   \n",
      "B_g                                               2.789117e+03 -1.857376e+05   \n",
      "t_0_g                                             3.297035e+02 -6.535659e+03   \n",
      "t_fall_g                                          5.752004e+07 -9.281545e+07   \n",
      "t_rise_g                                          2.540651e+02 -1.684060e+04   \n",
      "A_r                                               1.040686e+24 -2.955698e+15   \n",
      "B_r                                               1.074974e+05 -9.430196e+06   \n",
      "t_0_r                                             3.295673e+02 -5.843093e+03   \n",
      "t_fall_r                                          1.503842e+08 -2.173155e+09   \n",
      "t_rise_r                                          4.949786e+02 -2.869474e+04   \n",
      "A_i                                               3.652274e+10 -1.778728e+05   \n",
      "B_i                                               2.728974e+04 -1.410504e+06   \n",
      "t_0_i                                             3.065890e+02 -5.271084e+03   \n",
      "t_fall_i                                          4.743604e+08 -4.302124e+09   \n",
      "t_rise_i                                          7.491546e+02 -3.853234e+04   \n",
      "A_z                                               6.602216e+21 -2.683817e+05   \n",
      "B_z                                               6.753489e+04 -5.720279e+06   \n",
      "t_0_z                                             3.554371e+02 -1.364860e+04   \n",
      "t_fall_z                                          5.610900e+08 -1.289611e+09   \n",
      "t_rise_z                                          9.045662e+02 -4.889290e+04   \n",
      "\n",
      "                                                           25%            50%  \\\n",
      "flux_min                                            -97.748140     -63.014893   \n",
      "flux_max                                             92.712827     166.902748   \n",
      "flux_mean                                             1.952291       7.054367   \n",
      "flux_median                                           0.233296       0.993680   \n",
      "flux_std                                             22.534987      35.915890   \n",
      "flux_skew                                             0.656051       2.149175   \n",
      "flux_err_min                                          1.046307       1.516784   \n",
      "flux_err_max                                         45.324589      54.928312   \n",
      "flux_err_mean                                         4.421238      12.828899   \n",
      "flux_err_median                                       3.257738       9.393351   \n",
      "flux_err_std                                          4.231706      10.814982   \n",
      "flux_err_skew                                         1.213292       1.499814   \n",
      "detected_mean                                         0.029814       0.071429   \n",
      "flux_ratio_sq_sum                                  1163.388509    4659.903047   \n",
      "flux_ratio_sq_skew                                    4.600050       6.557691   \n",
      "flux_by_flux_ratio_sq_sum                         18387.658261  245633.617776   \n",
      "flux_by_flux_ratio_sq_skew                            4.371290       7.062250   \n",
      "flux_w_mean                                          25.136668      83.231919   \n",
      "flux_diff1                                          160.170960     255.436458   \n",
      "flux_diff2                                           12.395337      22.569265   \n",
      "flux_diff3                                            1.494130       1.986227   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"              18.305239      48.102023   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"              23.458444      46.555761   \n",
      "0__kurtosis                                          -0.376250       0.634380   \n",
      "0__skewness                                          -0.246774       0.316416   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"              17.174671      79.951323   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"              15.636640      64.729119   \n",
      "1__kurtosis                                           0.105205       2.999472   \n",
      "1__skewness                                           0.038536       1.276447   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"              76.046479     246.346179   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"              63.231898     198.328051   \n",
      "2__kurtosis                                           0.802989       4.821736   \n",
      "2__skewness                                           0.427805       1.947666   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"              93.025052     275.749970   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"              79.165062     222.339731   \n",
      "3__kurtosis                                           0.645124       3.949253   \n",
      "3__skewness                                           0.357315       1.817056   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"             144.869882     359.393182   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"             129.255762     301.527388   \n",
      "4__kurtosis                                           0.641737       3.415452   \n",
      "4__skewness                                           0.297040       1.609730   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"             149.471881     367.008601   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"             166.007396     330.686203   \n",
      "5__kurtosis                                           0.163575       1.504293   \n",
      "5__skewness                                           0.013215       0.718874   \n",
      "flux__length                                        122.000000     136.000000   \n",
      "flux__longest_strike_above_mean                       7.000000      10.000000   \n",
      "flux__longest_strike_below_mean                       9.000000      16.000000   \n",
      "flux__mean_abs_change                                15.011102      20.554584   \n",
      "flux__mean_change                                    -0.101453       0.001611   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean      3.000000       5.000000   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean     44.000000      79.500000   \n",
      "mjd__mean_abs_change                                  4.218475       7.842496   \n",
      "mjd__mean_change                                      4.218475       7.842496   \n",
      "mjd_diff_det                                         40.882500      95.774050   \n",
      "index                                              1961.750000    3923.500000   \n",
      "hostgal_photoz                                        0.000000       0.210300   \n",
      "hostgal_photoz_err                                    0.000000       0.018000   \n",
      "distmod                                              39.845250      41.167900   \n",
      "mwebv                                                 0.018000       0.032000   \n",
      "haversine                                             1.164021       1.690880   \n",
      "latlon1                                              -2.941054      -0.987176   \n",
      "hostgal_photoz_certain                                0.000000       0.236039   \n",
      "A_g                                                  18.661756      41.449736   \n",
      "B_g                                                  -5.026541      -0.840145   \n",
      "t_0_g                                               280.218869     440.646736   \n",
      "t_fall_g                                              3.460001      17.158744   \n",
      "t_rise_g                                             -2.422015      -0.439098   \n",
      "A_r                                                  16.485055      61.371916   \n",
      "B_r                                                  -1.431489      -0.221207   \n",
      "t_0_r                                               123.808415     408.938068   \n",
      "t_fall_r                                              7.392587      21.592463   \n",
      "t_rise_r                                             -3.606525      -0.797385   \n",
      "A_i                                                  32.007779     107.528049   \n",
      "B_i                                                  -1.282250      -0.098732   \n",
      "t_0_i                                               115.731104     388.928424   \n",
      "t_fall_i                                             15.336549      28.711572   \n",
      "t_rise_i                                             -4.010492      -0.706039   \n",
      "A_z                                                  41.732048     128.745261   \n",
      "B_z                                                  -2.202025      -0.179800   \n",
      "t_0_z                                               176.199992     415.309100   \n",
      "t_fall_z                                             17.049257      31.709991   \n",
      "t_rise_z                                             -4.631864      -1.001784   \n",
      "\n",
      "                                                           75%           max  \n",
      "flux_min                                         -3.720073e+01  5.109941e+02  \n",
      "flux_max                                          3.650707e+02  2.432809e+06  \n",
      "flux_mean                                         1.746234e+01  1.403881e+05  \n",
      "flux_median                                       2.535580e+00  3.087974e+04  \n",
      "flux_std                                          7.584766e+01  2.796894e+05  \n",
      "flux_skew                                         3.441721e+00  1.849729e+01  \n",
      "flux_err_min                                      2.166468e+00  3.276297e+02  \n",
      "flux_err_max                                      6.349475e+01  2.234069e+06  \n",
      "flux_err_mean                                     1.551099e+01  1.047130e+05  \n",
      "flux_err_median                                   1.195244e+01  3.853605e+03  \n",
      "flux_err_std                                      1.239876e+01  3.605993e+05  \n",
      "flux_err_skew                                     1.803128e+00  1.378938e+01  \n",
      "detected_mean                                     1.666667e-01  1.000000e+00  \n",
      "flux_ratio_sq_sum                                 2.592588e+04  1.086076e+07  \n",
      "flux_ratio_sq_skew                                8.634171e+00  1.873359e+01  \n",
      "flux_by_flux_ratio_sq_sum                         2.979438e+06  1.570616e+11  \n",
      "flux_by_flux_ratio_sq_skew                        9.486608e+00  1.876046e+01  \n",
      "flux_w_mean                                       2.013387e+02  1.793980e+05  \n",
      "flux_diff1                                        5.078297e+02  3.582197e+06  \n",
      "flux_diff2                                        4.301897e+01  6.077639e+04  \n",
      "flux_diff3                                        3.085920e+00  2.836464e+03  \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           1.675728e+02  2.230438e+06  \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           1.393153e+02  4.631342e+06  \n",
      "0__kurtosis                                       2.905992e+00  7.155293e+01  \n",
      "0__skewness                                       1.276045e+00  8.446201e+00  \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           3.871591e+02  6.352250e+05  \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           2.915286e+02  4.443702e+05  \n",
      "1__kurtosis                                       8.990190e+00  5.777428e+01  \n",
      "1__skewness                                       2.842803e+00  7.593994e+00  \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           8.201768e+02  3.741323e+06  \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           6.110753e+02  2.428124e+06  \n",
      "2__kurtosis                                       1.183584e+01  5.798279e+01  \n",
      "2__skewness                                       3.143396e+00  7.609116e+00  \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           8.380184e+02  3.265140e+06  \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           6.382118e+02  2.145546e+06  \n",
      "3__kurtosis                                       9.605804e+00  5.771809e+01  \n",
      "3__skewness                                       2.848140e+00  7.588468e+00  \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           9.435420e+02  3.667709e+06  \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7.284919e+02  2.625974e+06  \n",
      "4__kurtosis                                       8.633458e+00  5.792441e+01  \n",
      "4__skewness                                       2.623668e+00  7.608456e+00  \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           9.050253e+02  4.046475e+06  \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7.192184e+02  2.876665e+06  \n",
      "5__kurtosis                                       4.586256e+00  5.610575e+01  \n",
      "5__skewness                                       1.783578e+00  7.466092e+00  \n",
      "flux__length                                      2.550000e+02  3.520000e+02  \n",
      "flux__longest_strike_above_mean                   1.600000e+01  2.310000e+02  \n",
      "flux__longest_strike_below_mean                   3.525000e+01  3.310000e+02  \n",
      "flux__mean_abs_change                             3.197212e+01  1.172606e+05  \n",
      "flux__mean_change                                 1.184764e-01  2.246845e+03  \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  8.000000e+00  2.830000e+02  \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  1.140000e+02  3.460000e+02  \n",
      "mjd__mean_abs_change                              1.900680e+01  1.071030e+03  \n",
      "mjd__mean_change                                  1.900680e+01  1.071030e+03  \n",
      "mjd_diff_det                                      4.740591e+02  1.092845e+03  \n",
      "index                                             5.885250e+03  7.847000e+03  \n",
      "hostgal_photoz                                    4.312000e-01  2.999400e+00  \n",
      "hostgal_photoz_err                                1.223750e-01  1.734800e+00  \n",
      "distmod                                           4.239855e+01  4.702560e+01  \n",
      "mwebv                                             7.600000e-02  2.747000e+00  \n",
      "haversine                                         1.978942e+00  2.119827e+00  \n",
      "latlon1                                           2.311904e+00  8.805560e+00  \n",
      "hostgal_photoz_certain                            5.619134e-01  1.192976e+01  \n",
      "A_g                                               1.419656e+02  4.836684e+26  \n",
      "B_g                                               3.899379e-01  5.682246e+04  \n",
      "t_0_g                                             7.348732e+02  3.068339e+03  \n",
      "t_fall_g                                          4.004376e+01  3.392325e+09  \n",
      "t_rise_g                                         -2.216456e-02  6.531937e+01  \n",
      "A_r                                               2.487128e+02  9.213452e+25  \n",
      "B_r                                               4.519835e-01  1.804864e+04  \n",
      "t_0_r                                             7.228707e+02  4.881238e+03  \n",
      "t_fall_r                                          4.051180e+01  1.240937e+10  \n",
      "t_rise_r                                         -7.571188e-02  3.209530e+03  \n",
      "A_i                                               3.202958e+02  2.292537e+12  \n",
      "B_i                                               6.772686e-01  6.634931e+04  \n",
      "t_0_i                                             6.910987e+02  2.399981e+03  \n",
      "t_fall_i                                          4.969416e+01  2.817913e+10  \n",
      "t_rise_i                                         -1.717565e-02  2.497953e+04  \n",
      "A_z                                               3.502834e+02  5.848833e+23  \n",
      "B_z                                               8.671185e-01  2.321404e+05  \n",
      "t_0_z                                             7.296271e+02  7.511444e+03  \n",
      "t_fall_z                                          5.017892e+01  3.832841e+10  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rise_z                                         -4.521134e-02  2.510173e+04  \n",
      "[0]\tvalidation_0-merror:0.399107\tvalidation_1-merror:0.467199\tvalidation_0-wloss:2.54322\tvalidation_1-wloss:2.56077\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.119792\tvalidation_1-merror:0.222518\tvalidation_0-wloss:0.512934\tvalidation_1-wloss:0.812385\n",
      "[200]\tvalidation_0-merror:0.071429\tvalidation_1-merror:0.203014\tvalidation_0-wloss:0.247132\tvalidation_1-wloss:0.625389\n",
      "[300]\tvalidation_0-merror:0.039435\tvalidation_1-merror:0.190603\tvalidation_0-wloss:0.154997\tvalidation_1-wloss:0.590383\n",
      "Stopping. Best iteration:\n",
      "[342]\tvalidation_0-merror:0.031101\tvalidation_1-merror:0.18883\tvalidation_0-wloss:0.133535\tvalidation_1-wloss:0.588611\n",
      "\n",
      "no 1-fold loss: 0.5886109258855621\n",
      "[0]\tvalidation_0-merror:0.391434\tvalidation_1-merror:0.428826\tvalidation_0-wloss:2.54106\tvalidation_1-wloss:2.54927\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.121654\tvalidation_1-merror:0.215302\tvalidation_0-wloss:0.511789\tvalidation_1-wloss:0.809971\n",
      "[200]\tvalidation_0-merror:0.068114\tvalidation_1-merror:0.196619\tvalidation_0-wloss:0.245223\tvalidation_1-wloss:0.636206\n",
      "[300]\tvalidation_0-merror:0.039857\tvalidation_1-merror:0.191281\tvalidation_0-wloss:0.153374\tvalidation_1-wloss:0.608743\n",
      "Stopping. Best iteration:\n",
      "[312]\tvalidation_0-merror:0.036883\tvalidation_1-merror:0.19484\tvalidation_0-wloss:0.146563\tvalidation_1-wloss:0.608047\n",
      "\n",
      "no 2-fold loss: 0.6080466198302609\n",
      "[0]\tvalidation_0-merror:0.371599\tvalidation_1-merror:0.442565\tvalidation_0-wloss:2.53637\tvalidation_1-wloss:2.54911\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.11881\tvalidation_1-merror:0.235975\tvalidation_0-wloss:0.503594\tvalidation_1-wloss:0.914879\n",
      "[200]\tvalidation_0-merror:0.071673\tvalidation_1-merror:0.219056\tvalidation_0-wloss:0.241781\tvalidation_1-wloss:0.741654\n",
      "[300]\tvalidation_0-merror:0.03881\tvalidation_1-merror:0.198575\tvalidation_0-wloss:0.15201\tvalidation_1-wloss:0.713565\n",
      "Stopping. Best iteration:\n",
      "[316]\tvalidation_0-merror:0.035093\tvalidation_1-merror:0.198575\tvalidation_0-wloss:0.14296\tvalidation_1-wloss:0.713213\n",
      "\n",
      "no 3-fold loss: 0.7132127833741063\n",
      "[0]\tvalidation_0-merror:0.394975\tvalidation_1-merror:0.46744\tvalidation_0-wloss:2.53384\tvalidation_1-wloss:2.54966\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.121154\tvalidation_1-merror:0.223907\tvalidation_0-wloss:0.506815\tvalidation_1-wloss:0.821158\n",
      "[200]\tvalidation_0-merror:0.067787\tvalidation_1-merror:0.206066\tvalidation_0-wloss:0.2444\tvalidation_1-wloss:0.627156\n",
      "[300]\tvalidation_0-merror:0.03642\tvalidation_1-merror:0.193577\tvalidation_0-wloss:0.152615\tvalidation_1-wloss:0.588984\n",
      "[400]\tvalidation_0-merror:0.020663\tvalidation_1-merror:0.189117\tvalidation_0-wloss:0.109568\tvalidation_1-wloss:0.586954\n",
      "Stopping. Best iteration:\n",
      "[352]\tvalidation_0-merror:0.027798\tvalidation_1-merror:0.190901\tvalidation_0-wloss:0.126941\tvalidation_1-wloss:0.586044\n",
      "\n",
      "no 4-fold loss: 0.5860441961526597\n",
      "[0]\tvalidation_0-merror:0.379254\tvalidation_1-merror:0.432529\tvalidation_0-wloss:2.53292\tvalidation_1-wloss:2.54293\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.119929\tvalidation_1-merror:0.234138\tvalidation_0-wloss:0.509183\tvalidation_1-wloss:0.851383\n",
      "[200]\tvalidation_0-merror:0.070293\tvalidation_1-merror:0.211796\tvalidation_0-wloss:0.246547\tvalidation_1-wloss:0.666741\n",
      "[300]\tvalidation_0-merror:0.039679\tvalidation_1-merror:0.198391\tvalidation_0-wloss:0.155021\tvalidation_1-wloss:0.62671\n",
      "Stopping. Best iteration:\n",
      "[316]\tvalidation_0-merror:0.035518\tvalidation_1-merror:0.193923\tvalidation_0-wloss:0.146164\tvalidation_1-wloss:0.624594\n",
      "\n",
      "no 5-fold loss: 0.6245943789074306\n",
      "[0]\tvalidation_0-merror:0.404398\tvalidation_1-merror:0.457475\tvalidation_0-wloss:2.53895\tvalidation_1-wloss:2.55438\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.118705\tvalidation_1-merror:0.229185\tvalidation_0-wloss:0.512783\tvalidation_1-wloss:0.82807\n",
      "[200]\tvalidation_0-merror:0.069826\tvalidation_1-merror:0.203223\tvalidation_0-wloss:0.247536\tvalidation_1-wloss:0.622617\n",
      "[300]\tvalidation_0-merror:0.038182\tvalidation_1-merror:0.197851\tvalidation_0-wloss:0.155889\tvalidation_1-wloss:0.584209\n",
      "[400]\tvalidation_0-merror:0.022285\tvalidation_1-merror:0.190689\tvalidation_0-wloss:0.112084\tvalidation_1-wloss:0.580335\n",
      "Stopping. Best iteration:\n",
      "[385]\tvalidation_0-merror:0.024068\tvalidation_1-merror:0.190689\tvalidation_0-wloss:0.116991\tvalidation_1-wloss:0.579635\n",
      "\n",
      "no 6-fold loss: 0.5796345813851677\n",
      "[0]\tvalidation_0-merror:0.387552\tvalidation_1-merror:0.450717\tvalidation_0-wloss:2.5352\tvalidation_1-wloss:2.5503\n",
      "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-merror:0.121064\tvalidation_1-merror:0.243728\tvalidation_0-wloss:0.508441\tvalidation_1-wloss:0.882667\n",
      "[200]\tvalidation_0-merror:0.072787\tvalidation_1-merror:0.213262\tvalidation_0-wloss:0.245143\tvalidation_1-wloss:0.704507\n",
      "[300]\tvalidation_0-merror:0.040553\tvalidation_1-merror:0.203405\tvalidation_0-wloss:0.153829\tvalidation_1-wloss:0.680373\n",
      "[400]\tvalidation_0-merror:0.022876\tvalidation_1-merror:0.193548\tvalidation_0-wloss:0.110781\tvalidation_1-wloss:0.674677\n",
      "Stopping. Best iteration:\n",
      "[374]\tvalidation_0-merror:0.025401\tvalidation_1-merror:0.192652\tvalidation_0-wloss:0.119389\tvalidation_1-wloss:0.673465\n",
      "\n",
      "no 7-fold loss: 0.6734645545602517\n",
      "MULTI WEIGHTED LOG LOSS: 0.62399\n",
      "save to subm_0.623994_2018-12-14-14-38.csv\n",
      "(3492890, 14)\n",
      "(3492891, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:13<00:00,  1.50it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:05<00:00,  3.72it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:02<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        5000000 done in   1.4 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:13<00:00,  1.52it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:05<00:00,  3.77it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:03<00:00,  6.54it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:02<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10000000 done in   2.4 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:29<00:00,  1.47s/it]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:06<00:00,  3.02it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:04<00:00,  4.41it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:03<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       15000000 done in   4.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:32<00:00,  1.62s/it]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:06<00:00,  2.87it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:04<00:00,  4.32it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:03<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       20000000 done in   6.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:32<00:00,  1.63s/it]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:06<00:00,  2.90it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:04<00:00,  4.21it/s]\n",
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:03<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       25000000 done in   8.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████| 20/20 [00:32<00:00,  1.62s/it]\n",
      "Feature Extraction:   0%|                               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    719\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-889f92aff44c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-889f92aff44c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(argc, argv)\u001b[0m\n\u001b[0;32m    636\u001b[0m                  \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m                  \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m                  chunks=5000000)\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-889f92aff44c>\u001b[0m in \u001b[0;36mprocess_test\u001b[1;34m(clfs, features, featurize_configs, scaler, train_mean, filename, chunks)\u001b[0m\n\u001b[0;32m    469\u001b[0m                                  \u001b[0mfeaturize_configs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeaturize_configs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m                                  \u001b[0mtrain_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m                                  scaler=scaler)\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_c\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-889f92aff44c>\u001b[0m in \u001b[0;36mpredict_chunk\u001b[1;34m(df_, clfs_, meta_, features, featurize_configs, train_mean, scaler)\u001b[0m\n\u001b[0;32m    414\u001b[0m     full_test = featurize(df_, meta_, \n\u001b[0;32m    415\u001b[0m                           \u001b[0mfeaturize_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'aggs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                           featurize_configs['fcp'])\n\u001b[0m\u001b[0;32m    417\u001b[0m     \u001b[0mfull_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-889f92aff44c>\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(df, df_meta, aggs, fcp, n_jobs)\u001b[0m\n\u001b[0;32m    153\u001b[0m                                       \u001b[0mcolumn_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                                       \u001b[0mcolumn_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'flux'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                                       default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor)\u001b[0m\n\u001b[0;32m    157\u001b[0m                                 \u001b[0mdefault_fc_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_fc_parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                                 \u001b[0mkind_to_fc_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind_to_fc_parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                                 distributor=distributor)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# Impute the result if requested\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py\u001b[0m in \u001b[0;36m_do_extraction\u001b[1;34m(df, column_id, column_value, column_kind, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, distributor)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_fc_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_fc_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind_to_fc_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind_to_fc_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     result = distributor.map_reduce(_do_extraction_on_chunk, data=data_in_chunks, chunk_size=chunk_size,\n\u001b[1;32m--> 240\u001b[1;33m                                     function_kwargs=kwargs)\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0mdistributor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tsfresh\\utilities\\distribution.py\u001b[0m in \u001b[0;36mmap_reduce\u001b[1;34m(self, map_function, data, function_kwargs, chunk_size, data_length)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_function_with_partly_reduce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    928\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    722\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is forked from iprapas's notebook \n",
    "#    https://www.kaggle.com/iprapas/ideas-from-kernels-and-discussion-lb-1-135\n",
    "#    https://www.kaggle.com/ogrellier/plasticc-in-a-kernel-meta-and-data\n",
    "#    https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70908\n",
    "#    https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification\n",
    "#\n",
    "# obtained SMOTE idea from\n",
    "#    https://www.kaggle.com/jimpsull/collaboratingwithkagglecommunity-1-037-lb\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import gc; gc.enable()\n",
    "from functools import partial, wraps\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import itertools\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "@jit\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq, \n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n",
    "        index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux], axis=1)\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "    \n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,       \n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux_agg], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "def featurize(df, df_meta, aggs, fcp, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "    \n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df = process_flux_agg(agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(df, \n",
    "                                               column_id='object_id', \n",
    "                                               column_sort='mjd', \n",
    "                                               column_kind='passband', \n",
    "                                               column_value='flux', \n",
    "                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux', \n",
    "                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux_by_flux_ratio_sq', \n",
    "                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "    agg_df_mjd = extract_features(df_det, \n",
    "                                  column_id='object_id', \n",
    "                                  column_value='mjd', \n",
    "                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    \n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True) \n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)      \n",
    "    agg_df_ts = pd.concat([agg_df, \n",
    "                           agg_df_ts_flux_passband, \n",
    "                           agg_df_ts_flux, \n",
    "                           agg_df_ts_flux_by_flux_ratio_sq, \n",
    "                           agg_df_mjd], axis=1).reset_index()\n",
    "    \n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_meta(filename, filename2):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "    sn_df = pd.read_csv(filename2)\n",
    "    \n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    \n",
    "    print(meta_df.shape)\n",
    "    \n",
    "    meta_df = meta_df.reset_index().merge(\n",
    "        right=sn_df,\n",
    "        how='outer',\n",
    "        on='object_id'\n",
    "    )\n",
    "    \n",
    "    print(meta_df.shape)\n",
    "    \n",
    "    return meta_df\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds, classes, class_weights):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgbm_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"  \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def xgb_multi_weighted_logloss(y_predicted, y_true, classes, class_weights):\n",
    "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted, \n",
    "                                  classes, class_weights)\n",
    "    return 'wloss', loss\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    return importances_\n",
    "\n",
    "\n",
    "def xgb_modeling_cross_validation(params,\n",
    "                                  full_train, \n",
    "                                  y, \n",
    "                                  classes, \n",
    "                                  class_weights, \n",
    "                                  nr_fold=5, \n",
    "                                  random_state=1):\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    # loss function\n",
    "    func_loss = partial(xgb_multi_weighted_logloss, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights)\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=func_loss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('xgb_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train, \n",
    "                                   y, \n",
    "                                   classes, \n",
    "                                   class_weights, \n",
    "                                   nr_fold=5, \n",
    "                                   random_state=1):\n",
    "\n",
    "    sample_sub = pd.read_csv('sample_submission.csv')\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub;gc.collect()\n",
    "        \n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        sm = SMOTE(k_neighbors=7, n_jobs=4, random_state=42)\n",
    "        trn_x, trn_y = sm.fit_resample(trn_x, trn_y)\n",
    "        trn_x = pd.DataFrame(trn_x, columns=full_train.columns)\n",
    "        trn_y = pd.Series(trn_y)\n",
    "\n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgbm_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=100,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = confusion_matrix(val_y, np.argmax(oof_preds[val_, :],axis=-1))\n",
    "        cnf_matrix = cnf_matrix[~np.all(cnf_matrix == 0, axis=1)]\n",
    "        cnf_matrix = cnf_matrix[:, ~np.all(cnf_matrix == 0, axis=0)]\n",
    "        np.set_printoptions(precision=2)\n",
    "        \n",
    "    \n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(figsize=(12,12))\n",
    "        foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('lgbm_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "    \n",
    "def gen_unknown(series):\n",
    "    \"\"\"\n",
    "    from https://www.kaggle.com/c/PLAsTiCC-2018/discussion/72104#426782\n",
    "    \"\"\"\n",
    "    return (0.5 + 0.5 * series.median()+ 0.25 * series.mean() - 0.5 * series.max() ** 3) / 2.\n",
    "    \n",
    "    \n",
    "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean, scaler):\n",
    "    \n",
    "    # process all features\n",
    "    full_test = featurize(df_, meta_, \n",
    "                          featurize_configs['aggs'], \n",
    "                          featurize_configs['fcp'])\n",
    "    full_test.fillna(train_mean, inplace=True)\n",
    "    ind = full_test['object_id']\n",
    "    full_test = pd.DataFrame(scaler.transform(full_test[features]), columns=features, index=full_test.index)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test)\n",
    "            \n",
    "    preds_ = preds_ / len(clfs_)\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n",
    "    preds_99 = preds_df_.apply(lambda x: gen_unknown(x), axis=1)\n",
    "    preds_df_['object_id'] = ind.values\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def process_test(clfs, \n",
    "                 features, \n",
    "                 featurize_configs, \n",
    "                 scaler,\n",
    "                 train_mean,\n",
    "                 filename='predictions.csv',\n",
    "                 chunks=5000000):\n",
    "    start = time.time()\n",
    "\n",
    "    meta_test = process_meta('test_set_metadata.csv', 'supernova_test.csv')\n",
    "    # meta_test.set_index('object_id',inplace=True)\n",
    "\n",
    "    remain_df = None\n",
    "    for i_c, df in enumerate(pd.read_csv('test_set.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "        \n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "        if remain_df is None:\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "        \n",
    "        preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clfs,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=features,\n",
    "                                 featurize_configs=featurize_configs,\n",
    "                                 train_mean=train_mean, \n",
    "                                 scaler=scaler)\n",
    "    \n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv(filename, header=True, mode='a', index=False)\n",
    "        else:\n",
    "            preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    \n",
    "        del preds_df\n",
    "        gc.collect()\n",
    "        print('{:15d} done in {:5.1f} minutes' .format(\n",
    "                chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "        \n",
    "    # Compute last object in remain_df\n",
    "    preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=features,\n",
    "                             featurize_configs=featurize_configs,\n",
    "                             train_mean=train_mean, \n",
    "                             scaler=scaler)\n",
    "        \n",
    "    preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def main(argc, argv):\n",
    "    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity    \n",
    "    \n",
    "    # agg features\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "    \n",
    "    # tsfresh features\n",
    "    fcp = {\n",
    "        'flux': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "            'length': None,\n",
    "        },\n",
    "                \n",
    "        'flux_by_flux_ratio_sq': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,       \n",
    "        },\n",
    "                \n",
    "        'flux_passband': {\n",
    "            'fft_coefficient': [\n",
    "                    {'coeff': 0, 'attr': 'abs'}, \n",
    "                    {'coeff': 1, 'attr': 'abs'}\n",
    "                ],\n",
    "            'kurtosis' : None, \n",
    "            'skewness' : None,\n",
    "        },\n",
    "                \n",
    "        'mjd': {\n",
    "            'maximum': None, \n",
    "            'minimum': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    best_params = {\n",
    "            'device': 'cpu', \n",
    "            'objective': 'multiclass', \n",
    "            'num_class': 14, \n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': -1, \n",
    "            'max_depth': 7, \n",
    "            'n_estimators': 500, \n",
    "            'subsample_freq': 2, \n",
    "            'subsample_for_bin': 5000, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 1.0, \n",
    "            'cat_smooth': 59.5, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'metric': 'multi_logloss', \n",
    "            'xgboost_dart_mode': False, \n",
    "            'uniform_drop': False, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'drop_rate': 0.173, \n",
    "            'learning_rate': 0.0267, \n",
    "            'max_drop': 5, \n",
    "            'min_child_samples': 10, \n",
    "            'min_child_weight': 100.0, \n",
    "            'min_split_gain': 0.1, \n",
    "            'num_leaves': 7, \n",
    "            'reg_alpha': 0.1, \n",
    "            'reg_lambda': 0.00023, \n",
    "            'skip_drop': 0.44, \n",
    "            'subsample': 0.75}\n",
    "\n",
    "    meta_train = process_meta('training_set_metadata_head_oldlab.csv', 'supernova_train.csv')\n",
    "    \n",
    "    train = pd.read_csv('training_set.csv')\n",
    "    full_train = featurize(train, meta_train, aggs, fcp)\n",
    "\n",
    "    if 'target' in full_train:\n",
    "        y = full_train['target']\n",
    "        del full_train['target']\n",
    "        \n",
    "    classes = sorted(y.unique())    \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    class_weights = {c: 1 for c in classes}\n",
    "    class_weights.update({c:2 for c in [64, 15]})\n",
    "    print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "    print(class_weights)\n",
    "    #sanity check: classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    #sanity check: class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    #if len(np.unique(y_true)) > 14:\n",
    "    #    classes.append(99)\n",
    "    #    class_weights[99] = 2\n",
    "    \n",
    "    if 'object_id' in full_train:\n",
    "        oof_df = full_train[['object_id']]\n",
    "        del full_train['object_id'] \n",
    "        #del full_train['distmod'] \n",
    "        del full_train['hostgal_specz']\n",
    "        del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "        del full_train['ddf']\n",
    "    \n",
    "    train_mean = full_train.mean(axis=0).to_frame().T\n",
    "    print(train_mean)\n",
    "    #train_mean.to_hdf('train_data.hdf5', 'data')\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    print(full_train.describe().T)\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    full_train = pd.DataFrame(scl.fit_transform(full_train), index=full_train.index, columns=full_train.columns)\n",
    "    full_train.fillna(0, inplace=True)\n",
    "\n",
    "    eval_func = partial(xgb_modeling_cross_validation, \n",
    "                        full_train=full_train, \n",
    "                        y=y, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights, \n",
    "                        nr_fold=7, \n",
    "                        random_state=7)\n",
    "\n",
    "    best_params.update({'n_estimators': 2000})\n",
    "    \n",
    "    # modeling from CV\n",
    "    clfs, score = eval_func(best_params)\n",
    "        \n",
    "    filename = 'subm_{:.6f}_{}.csv'.format(score, \n",
    "                     dt.now().strftime('%Y-%m-%d-%H-%M'))\n",
    "    print('save to {}'.format(filename))\n",
    "    # TEST\n",
    "    process_test(clfs, \n",
    "                 features=full_train.columns, \n",
    "                 featurize_configs={'aggs': aggs, 'fcp': fcp}, \n",
    "                 train_mean=train_mean, \n",
    "                 scaler=scl,\n",
    "                 filename=filename,\n",
    "                 chunks=5000000)\n",
    "        \n",
    "    z = pd.read_csv(filename)\n",
    "    print(\"Shape BEFORE grouping: {}\".format(z.shape))\n",
    "    z = z.groupby('object_id').mean()\n",
    "    print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "    z.to_csv('single_{}'.format(filename), index=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(len(sys.argv), sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
