{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is forked from iprapas's notebook \n",
    "#    https://www.kaggle.com/iprapas/ideas-from-kernels-and-discussion-lb-1-135\n",
    "#    https://www.kaggle.com/ogrellier/plasticc-in-a-kernel-meta-and-data\n",
    "#    https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70908\n",
    "#    https://www.kaggle.com/meaninglesslives/simple-neural-net-for-time-series-classification\n",
    "#\n",
    "# obtained SMOTE idea from\n",
    "#    https://www.kaggle.com/jimpsull/collaboratingwithkagglecommunity-1-037-lb\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import gc; gc.enable()\n",
    "from functools import partial, wraps\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "@jit\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq, \n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n",
    "        index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux], axis=1)\n",
    "\n",
    "\n",
    "@jit\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "    \n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,       \n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux_agg], axis=1)\n",
    "    \n",
    "\n",
    "def featurize(df, df_meta, aggs, fcp, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "    \n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df = process_flux_agg(agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(df, \n",
    "                                               column_id='object_id', \n",
    "                                               column_sort='mjd', \n",
    "                                               column_kind='passband', \n",
    "                                               column_value='flux', \n",
    "                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux', \n",
    "                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux_by_flux_ratio_sq', \n",
    "                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "    agg_df_mjd = extract_features(df_det, \n",
    "                                  column_id='object_id', \n",
    "                                  column_value='mjd', \n",
    "                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    \n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True) \n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)      \n",
    "    agg_df_ts = pd.concat([agg_df, \n",
    "                           agg_df_ts_flux_passband, \n",
    "                           agg_df_ts_flux, \n",
    "                           agg_df_ts_flux_by_flux_ratio_sq, \n",
    "                           agg_df_mjd], axis=1).reset_index()\n",
    "    \n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_meta(filename):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "    \n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds, classes, class_weights):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lgbm_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"  \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def xgb_multi_weighted_logloss(y_predicted, y_true, classes, class_weights):\n",
    "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted, \n",
    "                                  classes, class_weights)\n",
    "    return 'wloss', loss\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    return importances_\n",
    "\n",
    "\n",
    "def xgb_modeling_cross_validation(params,\n",
    "                                  full_train, \n",
    "                                  y, \n",
    "                                  classes, \n",
    "                                  class_weights, \n",
    "                                  nr_fold=5, \n",
    "                                  random_state=1):\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    # loss function\n",
    "    func_loss = partial(xgb_multi_weighted_logloss, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights)\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=func_loss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('xgb_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "\n",
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train, \n",
    "                                   y, \n",
    "                                   classes, \n",
    "                                   class_weights, \n",
    "                                   nr_fold=5, \n",
    "                                   random_state=1):\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        sm = SMOTE(k_neighbors=7, n_jobs=4, random_state=42)\n",
    "        trn_x, trn_y = sm.fit_resample(trn_x, trn_y)\n",
    "        trn_x = pd.DataFrame(trn_x, columns=full_train.columns)\n",
    "        trn_y = pd.Series(trn_y)\n",
    "\n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgbm_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=100,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('lgbm_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score\n",
    "\n",
    "    \n",
    "def gen_unknown(series):\n",
    "    \"\"\"\n",
    "    from https://www.kaggle.com/c/PLAsTiCC-2018/discussion/72104#426782\n",
    "    \"\"\"\n",
    "    return (0.5 + 0.5 * series.median()+ 0.25 * series.mean() - 0.5 * series.max() ** 3) / 2.\n",
    "    \n",
    "    \n",
    "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean, scaler):\n",
    "    \n",
    "    # process all features\n",
    "    full_test = featurize(df_, meta_, \n",
    "                          featurize_configs['aggs'], \n",
    "                          featurize_configs['fcp'])\n",
    "    full_test.fillna(train_mean, inplace=True)\n",
    "    ind = full_test['object_id']\n",
    "    full_test = pd.DataFrame(scaler.transform(full_test[features]), columns=features, index=full_test.index)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test)\n",
    "            \n",
    "    preds_ = preds_ / len(clfs_)\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n",
    "    preds_99 = preds_df_.apply(lambda x: gen_unknown(x), axis=1)\n",
    "    preds_df_['object_id'] = ind.values\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def process_test(clfs, \n",
    "                 features, \n",
    "                 featurize_configs, \n",
    "                 scaler,\n",
    "                 train_mean,\n",
    "                 filename='predictions.csv',\n",
    "                 chunks=5000000):\n",
    "    start = time.time()\n",
    "\n",
    "    meta_test = process_meta('../input/test_set_metadata.csv')\n",
    "    # meta_test.set_index('object_id',inplace=True)\n",
    "\n",
    "    remain_df = None\n",
    "    for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "        \n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "        if remain_df is None:\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "        \n",
    "        preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clfs,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=features,\n",
    "                                 featurize_configs=featurize_configs,\n",
    "                                 train_mean=train_mean, \n",
    "                                 scaler=scaler)\n",
    "    \n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv(filename, header=True, mode='a', index=False)\n",
    "        else:\n",
    "            preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    \n",
    "        del preds_df\n",
    "        gc.collect()\n",
    "        print('{:15d} done in {:5.1f} minutes' .format(\n",
    "                chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "        \n",
    "    # Compute last object in remain_df\n",
    "    preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=features,\n",
    "                             featurize_configs=featurize_configs,\n",
    "                             train_mean=train_mean, \n",
    "                             scaler=scaler)\n",
    "        \n",
    "    preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def main(argc, argv):\n",
    "    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity    \n",
    "    \n",
    "    # agg features\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum', 'skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "    \n",
    "    # tsfresh features\n",
    "    fcp = {\n",
    "        'flux': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "            'length': None,\n",
    "        },\n",
    "                \n",
    "        'flux_by_flux_ratio_sq': {\n",
    "            'longest_strike_above_mean': None,\n",
    "            'longest_strike_below_mean': None,       \n",
    "        },\n",
    "                \n",
    "        'flux_passband': {\n",
    "            'fft_coefficient': [\n",
    "                    {'coeff': 0, 'attr': 'abs'}, \n",
    "                    {'coeff': 1, 'attr': 'abs'}\n",
    "                ],\n",
    "            'kurtosis' : None, \n",
    "            'skewness' : None,\n",
    "        },\n",
    "                \n",
    "        'mjd': {\n",
    "            'maximum': None, \n",
    "            'minimum': None,\n",
    "            'mean_change': None,\n",
    "            'mean_abs_change': None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    best_params = {\n",
    "            'device': 'cpu', \n",
    "            'objective': 'multiclass', \n",
    "            'num_class': 14, \n",
    "            'boosting_type': 'gbdt', \n",
    "            'n_jobs': -1, \n",
    "            'max_depth': 7, \n",
    "            'n_estimators': 500, \n",
    "            'subsample_freq': 2, \n",
    "            'subsample_for_bin': 5000, \n",
    "            'min_data_per_group': 100, \n",
    "            'max_cat_to_onehot': 4, \n",
    "            'cat_l2': 1.0, \n",
    "            'cat_smooth': 59.5, \n",
    "            'max_cat_threshold': 32, \n",
    "            'metric_freq': 10, \n",
    "            'verbosity': -1, \n",
    "            'metric': 'multi_logloss', \n",
    "            'xgboost_dart_mode': False, \n",
    "            'uniform_drop': False, \n",
    "            'colsample_bytree': 0.5, \n",
    "            'drop_rate': 0.173, \n",
    "            'learning_rate': 0.0267, \n",
    "            'max_drop': 5, \n",
    "            'min_child_samples': 10, \n",
    "            'min_child_weight': 100.0, \n",
    "            'min_split_gain': 0.1, \n",
    "            'num_leaves': 7, \n",
    "            'reg_alpha': 0.1, \n",
    "            'reg_lambda': 0.00023, \n",
    "            'skip_drop': 0.44, \n",
    "            'subsample': 0.75}\n",
    "\n",
    "    meta_train = process_meta('../input/training_set_metadata.csv')\n",
    "    \n",
    "    train = pd.read_csv('../input/training_set.csv')\n",
    "    full_train = featurize(train, meta_train, aggs, fcp)\n",
    "\n",
    "    if 'target' in full_train:\n",
    "        y = full_train['target']\n",
    "        del full_train['target']\n",
    "        \n",
    "    classes = sorted(y.unique())    \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    class_weights = {c: 1 for c in classes}\n",
    "    class_weights.update({c:2 for c in [64, 15]})\n",
    "    print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "    print(class_weights)\n",
    "    #sanity check: classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    #sanity check: class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    #if len(np.unique(y_true)) > 14:\n",
    "    #    classes.append(99)\n",
    "    #    class_weights[99] = 2\n",
    "    \n",
    "    if 'object_id' in full_train:\n",
    "        oof_df = full_train[['object_id']]\n",
    "        del full_train['object_id'] \n",
    "        #del full_train['distmod'] \n",
    "        del full_train['hostgal_specz']\n",
    "        del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "        del full_train['ddf']\n",
    "    \n",
    "    train_mean = full_train.mean(axis=0).to_frame().T\n",
    "    print(train_mean)\n",
    "    #train_mean.to_hdf('train_data.hdf5', 'data')\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    print(full_train.describe().T)\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    full_train = pd.DataFrame(scl.fit_transform(full_train), index=full_train.index, columns=full_train.columns)\n",
    "    full_train.fillna(0, inplace=True)\n",
    "\n",
    "    eval_func = partial(lgbm_modeling_cross_validation, \n",
    "                        full_train=full_train, \n",
    "                        y=y, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights, \n",
    "                        nr_fold=7, \n",
    "                        random_state=7)\n",
    "\n",
    "    best_params.update({'n_estimators': 2000})\n",
    "    \n",
    "    # modeling from CV\n",
    "    clfs, score = eval_func(best_params)\n",
    "        \n",
    "    filename = 'subm_{:.6f}_{}.csv'.format(score, \n",
    "                     dt.now().strftime('%Y-%m-%d-%H-%M'))\n",
    "    print('save to {}'.format(filename))\n",
    "    # TEST\n",
    "    process_test(clfs, \n",
    "                 features=full_train.columns, \n",
    "                 featurize_configs={'aggs': aggs, 'fcp': fcp}, \n",
    "                 train_mean=train_mean, \n",
    "                 scaler=scl,\n",
    "                 filename=filename,\n",
    "                 chunks=5000000)\n",
    "        \n",
    "    z = pd.read_csv(filename)\n",
    "    print(\"Shape BEFORE grouping: {}\".format(z.shape))\n",
    "    z = z.groupby('object_id').mean()\n",
    "    print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "    z.to_csv('single_{}'.format(filename), index=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(len(sys.argv), sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
